{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Class\n",
    "\n",
    "## Overview\n",
    "The `Layer` class represents a fully connected layer (dense layer) in a neural network. It supports L1 and L2 regularization for both weights and biases, which helps in preventing overfitting.\n",
    "\n",
    "## Features\n",
    "- **Weight Initialization**: Small random values from a normal distribution.\n",
    "- **Bias Initialization**: Zeros.\n",
    "- **Forward Pass**: Computes the output as `output = inputs * weights + biases`.\n",
    "- **Backward Pass**: Computes gradients and includes L1/L2 regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_inputs,n_neurons,weight_regularizer_l1 = 0,weight_regularizer_l2 = 0,\n",
    "                 bias_regularizer_l1 = 0,bias_regularizer_l2 = 0):\n",
    "        self .weights = 0.01  * np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis = 0,keepdims = True)\n",
    "\n",
    "        if self.weight_regularizer_l2>0:\n",
    "            self.dweights+=2 * self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        if self.bias_regularizer_l2>0:\n",
    "            self.dbiases+=2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        if self.weight_regularizer_l1>0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights<0]=-1\n",
    "            self.dweights +=self.weight_regularizer_l1 * dL1\n",
    "\n",
    "        if self.bias_regularizer_l1>0:\n",
    "            dL1 = np.ones_like(self.bias)\n",
    "            dL1[self.biases<0]=-1\n",
    "            self.dbiases +=self.bias_regularizer_l1 * dL1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer_Dropout Class\n",
    "\n",
    "## Overview\n",
    "The `Layer_Dropout` class implements **Dropout Regularization** for neural networks.  \n",
    "Dropout helps prevent overfitting by randomly disabling neurons during training.\n",
    "\n",
    "## Features\n",
    "- **Dropout Probability**: Randomly drops a fraction of neurons.\n",
    "- **Forward Pass**: Applies dropout only during training.\n",
    "- **Backward Pass**: Propagates gradients only through active neurons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "\n",
    "    def __init__(self,dropout_rate):\n",
    "        self.active_rate = 1- dropout_rate\n",
    "\n",
    "    def forward(self,input):\n",
    "        self.inputs = input\n",
    "        self.binary_mask = np.random.binomial(1,self.active_rate,size = self.inputs.shape)/self.active_rate\n",
    "\n",
    "        self.output = self.inputs * self.binary_mask\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation_ReLU Class\n",
    "\n",
    "## Overview\n",
    "The `Activation_ReLU` class implements the **Rectified Linear Unit (ReLU)** activation function.  \n",
    "ReLU is commonly used in deep learning due to its simplicity and efficiency.\n",
    "\n",
    "## Features\n",
    "- **Forward Pass**: Applies the ReLU function (`max(0, x)`) to input values.\n",
    "- **Backward Pass**: Computes gradients for backpropagation, setting negative gradients to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs<=0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation_SoftMax\n",
    "\n",
    "## Overview\n",
    "The `Activation_SoftMax` class implements the **SoftMax activation function**, commonly used in the output layer of classification models.  \n",
    "SoftMax converts raw scores (logits) into **probabilities**, making it useful for multi-class classification.\n",
    "\n",
    "## Features\n",
    "- **Numerically Stable**: Uses `np.max(input, axis=1, keepdims=True)` to prevent overflow.\n",
    "- **Outputs Probabilities**: Converts input values into a probability distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_SoftMax:\n",
    "    def forward(self,input):\n",
    "        #make it stable by not letting large values grow very large due to exponentiation\n",
    "        exp_vals = np.exp(input - np.max(input,axis=1,keepdims= True))\n",
    "        #normalize it\n",
    "        probabilities = exp_vals/ np.sum(exp_vals,axis = 1,keepdims = True)\n",
    "        self.output = probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "## Overview\n",
    "This module implements **Regularization Loss**, **Categorical Cross-Entropy Loss**, and the combined **SoftMax + Cross-Entropy Loss** function for neural networks.  \n",
    "These loss functions help in training models effectively and improving generalization.\n",
    "\n",
    "## Features\n",
    "- **Regularization Loss**:\n",
    "  - Supports **L1 and L2 regularization** for weights and biases.\n",
    "- **Categorical Cross-Entropy Loss**:\n",
    "  - Computes negative log likelihood for classification.\n",
    "  - Supports both **one-hot encoded** and **integer class labels**.\n",
    "- **SoftMax + Cross-Entropy Loss**:\n",
    "  - Combines **SoftMax activation** and **Cross-Entropy loss** for better numerical stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "    def regularization_loss(self,layer):\n",
    "        regularization_loss = 0\n",
    "\n",
    "        if layer.weight_regularizer_l1 >0:\n",
    "            regularization_loss+=layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        if layer.weight_regularizer_l2 >0:\n",
    "            regularization_loss+=layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        if layer.bias_regularizer_l1 >0:\n",
    "            regularization_loss+=layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        if layer.bias_regularizer_l2 >0:\n",
    "            regularization_loss+=layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def calculate(self,output,truth):\n",
    "        sample_losses = self.forward(output,truth)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical_Cross_Entropy_Loss(Loss):\n",
    "    \n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7) ## to avoid predicted values becoing 0 or 1\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_pred_curr = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape)==2 :\n",
    "            temp= y_pred_clipped * y_true\n",
    "            y_pred_curr = np.sum(temp,axis=1)\n",
    "        \n",
    "        neg_log = -np.log(y_pred_curr)\n",
    "        return neg_log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_SoftMax()\n",
    "        self.loss = Categorical_Cross_Entropy_Loss()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer: Adam with Learning Rate Decay\n",
    "\n",
    "## Overview\n",
    "This module implements an **Adam optimizer** with **learning rate decay**.  \n",
    "Adam (Adaptive Moment Estimation) is a widely used optimization algorithm that combines momentum and adaptive learning rates for efficient training.\n",
    "\n",
    "## Features\n",
    "- **Adaptive learning rates** for each parameter.\n",
    "- **Momentum-based updates** (similar to RMSProp and Momentum SGD).\n",
    "- **Learning rate decay** to improve convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer_With_LearningRateDecay_And_ADAM:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer: RMSprop with Learning Rate Decay\n",
    "\n",
    "## Overview\n",
    "This module implements the **RMSprop (Root Mean Square Propagation) optimizer** with **learning rate decay**.  \n",
    "RMSprop is an adaptive gradient-based optimization method designed to **reduce oscillations** and **improve convergence speed**.\n",
    "\n",
    "## Features\n",
    "- **Adaptive learning rate** using exponentially weighted average squared gradients.\n",
    "- **Prevents large oscillations** in weight updates.\n",
    "- **Learning rate decay** to refine convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_With_LearningRateDecay_And_RMSprop:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                             (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                           (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer: AdaGrad with Learning Rate Decay\n",
    "\n",
    "## Overview\n",
    "This module implements the **AdaGrad (Adaptive Gradient) optimizer** with **learning rate decay**.  \n",
    "AdaGrad adapts the learning rate for each parameter based on past gradients, making it useful for **sparse data** and **online learning**.\n",
    "\n",
    "## Features\n",
    "- **Adaptive learning rate** – Updates parameters more for infrequent features.\n",
    "- **Learning rate decay** – Helps refine convergence over time.\n",
    "- **Prevents division by zero** – Uses `epsilon` for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer_GD_With_LearningRateDecay_And_ADAGRAD:\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "            layer.dweights / \\\n",
    "            (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "            layer.dbiases / \\\n",
    "            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer: Gradient Descent with Learning Rate Decay and Momentum\n",
    "\n",
    "## Overview\n",
    "This module implements the **Gradient Descent (GD) optimizer** with:\n",
    "- **Learning rate decay** – Adjusts the step size dynamically.\n",
    "- **Momentum** – Helps accelerate convergence and escape local minima.\n",
    "\n",
    "## Features\n",
    "- **Smooth updates** – Uses past gradients to refine updates.\n",
    "- **Adaptive learning rate** – Reduces step size over time.\n",
    "- **Prevents oscillations** – Momentum dampens erratic updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD_With_LearningRateDecay_And_Momentum:\n",
    "    def __init__(self, learning_rate = 1,decay = 0.,momentum = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay =decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1/(1 + self.decay * self.iterations))\n",
    "\n",
    "            \n",
    "    def update_params(self,layer):\n",
    "\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer,'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        else:\n",
    "            weight_updates =  -self.current_learning_ratelearning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer: Gradient Descent with Learning Rate Decay\n",
    "\n",
    "## Overview\n",
    "This module implements **Gradient Descent (GD) optimizer** with:\n",
    "- **Learning rate decay** – Gradually reduces step size over iterations.\n",
    "- **Basic gradient updates** – Uses standard GD without momentum.\n",
    "\n",
    "## Features\n",
    "- **Prevents overshooting** – Dynamically adjusts learning rate.\n",
    "- **Smooth convergence** – Reduces step size as training progresses.\n",
    "- **Easy to integrate** – Works with any neural network layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD_With_LearningRateDecay:\n",
    "    def __init__(self, learning_rate = 1,decay = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay =decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1/(1 + self.decay * self.iterations))\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer: Gradient Descent (Without Learning Rate Decay)\n",
    "\n",
    "## Overview\n",
    "This module implements **basic Gradient Descent (GD)** for optimizing neural network parameters.\n",
    "\n",
    "### 🔹 Features:\n",
    "- **Constant Learning Rate** – Uses a fixed step size.\n",
    "- **Simple & Efficient** – No extra computation for decay or momentum.\n",
    "- **Works for Small-Scale Models** – Best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD_Without_LearningRateDecay:\n",
    "    def __init__(self, learning_rate = 1):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 accuracy : 0.338 loss : 1.099\n",
      "epoch : 100 accuracy : 0.668 loss : 0.810\n",
      "epoch : 200 accuracy : 0.698 loss : 0.803\n",
      "epoch : 300 accuracy : 0.737 loss : 0.760\n",
      "epoch : 400 accuracy : 0.737 loss : 0.763\n",
      "epoch : 500 accuracy : 0.700 loss : 0.809\n",
      "epoch : 600 accuracy : 0.715 loss : 0.798\n",
      "epoch : 700 accuracy : 0.710 loss : 0.794\n",
      "epoch : 800 accuracy : 0.750 loss : 0.764\n",
      "epoch : 900 accuracy : 0.733 loss : 0.797\n",
      "epoch : 1000 accuracy : 0.745 loss : 0.767\n",
      "epoch : 1100 accuracy : 0.760 loss : 0.765\n",
      "epoch : 1200 accuracy : 0.732 loss : 0.780\n",
      "epoch : 1300 accuracy : 0.752 loss : 0.778\n",
      "epoch : 1400 accuracy : 0.747 loss : 0.771\n",
      "epoch : 1500 accuracy : 0.740 loss : 0.768\n",
      "epoch : 1600 accuracy : 0.750 loss : 0.765\n",
      "epoch : 1700 accuracy : 0.747 loss : 0.747\n",
      "epoch : 1800 accuracy : 0.733 loss : 0.779\n",
      "epoch : 1900 accuracy : 0.727 loss : 0.808\n",
      "epoch : 2000 accuracy : 0.743 loss : 0.769\n",
      "epoch : 2100 accuracy : 0.738 loss : 0.756\n",
      "epoch : 2200 accuracy : 0.740 loss : 0.795\n",
      "epoch : 2300 accuracy : 0.732 loss : 0.787\n",
      "epoch : 2400 accuracy : 0.738 loss : 0.778\n",
      "epoch : 2500 accuracy : 0.752 loss : 0.764\n",
      "epoch : 2600 accuracy : 0.735 loss : 0.761\n",
      "epoch : 2700 accuracy : 0.753 loss : 0.761\n",
      "epoch : 2800 accuracy : 0.743 loss : 0.755\n",
      "epoch : 2900 accuracy : 0.760 loss : 0.758\n",
      "epoch : 3000 accuracy : 0.757 loss : 0.755\n",
      "epoch : 3100 accuracy : 0.747 loss : 0.754\n",
      "epoch : 3200 accuracy : 0.718 loss : 0.770\n",
      "epoch : 3300 accuracy : 0.752 loss : 0.751\n",
      "epoch : 3400 accuracy : 0.737 loss : 0.761\n",
      "epoch : 3500 accuracy : 0.738 loss : 0.771\n",
      "epoch : 3600 accuracy : 0.743 loss : 0.748\n",
      "epoch : 3700 accuracy : 0.732 loss : 0.768\n",
      "epoch : 3800 accuracy : 0.758 loss : 0.751\n",
      "epoch : 3900 accuracy : 0.745 loss : 0.756\n",
      "epoch : 4000 accuracy : 0.728 loss : 0.784\n",
      "epoch : 4100 accuracy : 0.742 loss : 0.769\n",
      "epoch : 4200 accuracy : 0.723 loss : 0.765\n",
      "epoch : 4300 accuracy : 0.722 loss : 0.792\n",
      "epoch : 4400 accuracy : 0.735 loss : 0.769\n",
      "epoch : 4500 accuracy : 0.752 loss : 0.766\n",
      "epoch : 4600 accuracy : 0.758 loss : 0.752\n",
      "epoch : 4700 accuracy : 0.752 loss : 0.764\n",
      "epoch : 4800 accuracy : 0.740 loss : 0.771\n",
      "epoch : 4900 accuracy : 0.742 loss : 0.765\n",
      "epoch : 5000 accuracy : 0.767 loss : 0.758\n",
      "epoch : 5100 accuracy : 0.760 loss : 0.748\n",
      "epoch : 5200 accuracy : 0.750 loss : 0.753\n",
      "epoch : 5300 accuracy : 0.753 loss : 0.758\n",
      "epoch : 5400 accuracy : 0.752 loss : 0.760\n",
      "epoch : 5500 accuracy : 0.742 loss : 0.772\n",
      "epoch : 5600 accuracy : 0.738 loss : 0.753\n",
      "epoch : 5700 accuracy : 0.760 loss : 0.764\n",
      "epoch : 5800 accuracy : 0.752 loss : 0.761\n",
      "epoch : 5900 accuracy : 0.733 loss : 0.765\n",
      "epoch : 6000 accuracy : 0.770 loss : 0.744\n",
      "epoch : 6100 accuracy : 0.745 loss : 0.754\n",
      "epoch : 6200 accuracy : 0.743 loss : 0.774\n",
      "epoch : 6300 accuracy : 0.748 loss : 0.756\n",
      "epoch : 6400 accuracy : 0.743 loss : 0.787\n",
      "epoch : 6500 accuracy : 0.725 loss : 0.793\n",
      "epoch : 6600 accuracy : 0.747 loss : 0.768\n",
      "epoch : 6700 accuracy : 0.758 loss : 0.738\n",
      "epoch : 6800 accuracy : 0.738 loss : 0.767\n",
      "epoch : 6900 accuracy : 0.737 loss : 0.788\n",
      "epoch : 7000 accuracy : 0.755 loss : 0.751\n",
      "epoch : 7100 accuracy : 0.723 loss : 0.776\n",
      "epoch : 7200 accuracy : 0.770 loss : 0.735\n",
      "epoch : 7300 accuracy : 0.772 loss : 0.739\n",
      "epoch : 7400 accuracy : 0.725 loss : 0.775\n",
      "epoch : 7500 accuracy : 0.720 loss : 0.780\n",
      "epoch : 7600 accuracy : 0.755 loss : 0.753\n",
      "epoch : 7700 accuracy : 0.735 loss : 0.750\n",
      "epoch : 7800 accuracy : 0.755 loss : 0.763\n",
      "epoch : 7900 accuracy : 0.715 loss : 0.768\n",
      "epoch : 8000 accuracy : 0.737 loss : 0.789\n",
      "epoch : 8100 accuracy : 0.752 loss : 0.759\n",
      "epoch : 8200 accuracy : 0.748 loss : 0.780\n",
      "epoch : 8300 accuracy : 0.735 loss : 0.783\n",
      "epoch : 8400 accuracy : 0.753 loss : 0.767\n",
      "epoch : 8500 accuracy : 0.745 loss : 0.780\n",
      "epoch : 8600 accuracy : 0.738 loss : 0.769\n",
      "epoch : 8700 accuracy : 0.720 loss : 0.792\n",
      "epoch : 8800 accuracy : 0.738 loss : 0.737\n",
      "epoch : 8900 accuracy : 0.728 loss : 0.796\n",
      "epoch : 9000 accuracy : 0.757 loss : 0.754\n",
      "epoch : 9100 accuracy : 0.723 loss : 0.758\n",
      "epoch : 9200 accuracy : 0.767 loss : 0.736\n",
      "epoch : 9300 accuracy : 0.768 loss : 0.772\n",
      "epoch : 9400 accuracy : 0.743 loss : 0.773\n",
      "epoch : 9500 accuracy : 0.753 loss : 0.766\n",
      "epoch : 9600 accuracy : 0.753 loss : 0.759\n",
      "epoch : 9700 accuracy : 0.752 loss : 0.759\n",
      "epoch : 9800 accuracy : 0.760 loss : 0.745\n",
      "epoch : 9900 accuracy : 0.733 loss : 0.763\n",
      "epoch : 10000 accuracy : 0.767 loss : 0.747\n"
     ]
    }
   ],
   "source": [
    "X,y = spiral_data(samples=200,classes = 3)\n",
    "\n",
    "layer1 = Layer(2,128,weight_regularizer_l2=5e-4,bias_regularizer_l2=5e-4)\n",
    "activation1 = Activation_ReLU()\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "layer2 = Layer(128,3,weight_regularizer_l2=5e-5,bias_regularizer_l2=5e-5)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizerGD = Optimizer_With_LearningRateDecay_And_ADAM(learning_rate= 0.05, decay = 5e-5)\n",
    "\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    layer1.forward(X)\n",
    "    activation1.forward(layer1.output)\n",
    "    dropout1.forward(activation1.output)\n",
    "    layer2.forward(dropout1.output)\n",
    "\n",
    "    data_loss = loss_activation.forward(layer2.output,y)\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(layer1) + loss_activation.loss.regularization_loss(layer2)\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output,axis=1) ##calculates the index of maximum value along the row\n",
    "    if(len(y.shape)==2):\n",
    "        y = np.argmax(y,axis=1)\n",
    "    \n",
    "    if epoch%100 ==0:\n",
    "        accuracy = np.mean(predictions ==y)\n",
    "        print(f\"epoch : {epoch} \" + f\"accuracy : {accuracy:.3f} \" + f\"loss : {loss:.3f}\" )\n",
    "\n",
    "\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    layer2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(layer2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    layer1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizerGD.pre_update_params()\n",
    "    optimizerGD.update_params(layer1)\n",
    "    optimizerGD.update_params(layer2)\n",
    "    optimizerGD.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.780, loss: 0.645\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "layer1.forward(X_test)\n",
    "\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "layer2.forward(activation1.output)\n",
    "\n",
    "loss = loss_activation.forward(layer2.output, y_test)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    " y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Observations\n",
    "\n",
    "## Baseline (Adam Optimizer without Regularization)\n",
    "- Training Accuracy: 94.5%  \n",
    "- Testing Accuracy: 83.2%  \n",
    "- The model overfits, showing a high training accuracy but a lower test accuracy.\n",
    "\n",
    "## Adding L2 Regularization\n",
    "- Training Accuracy: 92.1%  \n",
    "- Testing Accuracy: 89.3%  \n",
    "- L2 regularization reduced overfitting and improved test accuracy.\n",
    "\n",
    "## Adding Dropout Regularization\n",
    "- Training Accuracy: 69.0%  \n",
    "- Testing Accuracy: 73.0%  \n",
    "- Dropout reduced training accuracy but improved generalization, leading to better test accuracy.\n",
    "\n",
    "## Conclusion\n",
    "- L2 regularization improved test accuracy while maintaining a reasonable training accuracy.  \n",
    "- Dropout further reduced overfitting, with a lower training accuracy but improved test accuracy.  \n",
    "- The best generalization was achieved with dropout, where test accuracy was higher than training accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy loss building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If class labels are directly given to us, this is how we extract the corresponding class predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n",
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[0.7,0.1,0.2],\n",
    "             [0.1,0.5,0.4],\n",
    "             [0.02,0.9,0.08]])\n",
    "class_targets = [0,1,1]\n",
    "print(a[ [0,1,2],class_targets])\n",
    "\n",
    "neg_log = -np.log(a[range(len(a)),class_targets])\n",
    "print(neg_log)\n",
    "avg_loss_normal = np.mean(neg_log)\n",
    "print(avg_loss_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if class labels are one hot encoded,this is how we extract the corresponding class predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[1,0,0],  # along the row class1, class2 , class3\n",
    "          [0,1,0],\n",
    "          [0,1,0]])\n",
    "\n",
    "y_pred =np.array([[0.7,0.1,0.2],\n",
    "             [0.1,0.5,0.4],\n",
    "             [0.02,0.9,0.08]])\n",
    "\n",
    "a = y_true * y_pred # element wise multiplication\n",
    "y_pred_curr = np.sum(a,axis=1)\n",
    "neg_log = -np.log(y_pred_curr)\n",
    "print(neg_log)\n",
    "avg_loss_one_hot = np.mean(neg_log)\n",
    "print(avg_loss_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking categorical cross entropy working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[1,0,0],  # along the row class1, class2 , class3\n",
    "          [0,1,0],\n",
    "          [0,1,0]])\n",
    "\n",
    "y_pred =np.array([[0.7,0.1,0.2],\n",
    "             [0.1,0.5,0.4],\n",
    "             [0.02,0.9,0.08]])\n",
    "\n",
    "loss_function = Categorical_Cross_Entropy_Loss()\n",
    "loss = loss_function.calculate(y_pred,y_true)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax function sample testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "inputs =  [[10,12,13],\n",
    "           [20,21,22],\n",
    "           [30,31,32]]\n",
    "x = inputs - np.max(inputs,axis=1,keepdims=True)\n",
    "probabilities = x/ np.sum(x,axis = 1,keepdims = True)\n",
    "\n",
    "print(np.sum(probabilities,axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Subtract the Maximum Value?\n",
    "\n",
    "#### 1. **Avoids Large Exponentials**\n",
    "- The exponential function (`e^x`) grows very fast for large values of `x`, leading to **overflow issues**.\n",
    "- If `inputs` contain large values, `np.exp(inputs)` might produce numbers too large for numerical computation.\n",
    "\n",
    "#### 2. **Keeps Values in a Stable Range**\n",
    "- By subtracting the **maximum value** in each row, the **largest number** in the row becomes `0`, and all other numbers become **negative**.\n",
    "- This ensures that when `np.exp()` is applied:\n",
    "  - The largest value remains `1`.\n",
    "  - Other values are in a **numerically stable range**.\n",
    "\n",
    "This technique is commonly used in **softmax activation** to improve numerical stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackPropagation on a single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Loss : 36.0\n",
      "Iteration 2 Loss : 33.872397424621624\n",
      "Iteration 3 Loss : 31.87054345809546\n",
      "Iteration 4 Loss : 29.98699091998773\n",
      "Iteration 5 Loss : 28.214761511794592\n",
      "Iteration 6 Loss : 26.54726775906168\n",
      "Iteration 7 Loss : 24.978326552541866\n",
      "Iteration 8 Loss : 23.5021050739742\n",
      "Iteration 9 Loss : 22.11313179151597\n",
      "Iteration 10 Loss : 20.806246424284897\n",
      "Iteration 11 Loss : 19.576596334671486\n",
      "Iteration 12 Loss : 18.41961908608719\n",
      "Iteration 13 Loss : 17.33101994032309\n",
      "Iteration 14 Loss : 16.306757070164853\n",
      "Iteration 15 Loss : 15.343027506224132\n",
      "Iteration 16 Loss : 14.436253786815284\n",
      "Iteration 17 Loss : 13.583071280700132\n",
      "Iteration 18 Loss : 12.780312744165439\n",
      "Iteration 19 Loss : 12.024995767388878\n",
      "Iteration 20 Loss : 11.314319082257104\n",
      "Iteration 21 Loss : 10.64564263994962\n",
      "Iteration 22 Loss : 10.016485041642266\n",
      "Iteration 23 Loss : 9.424510031713222\n",
      "Iteration 24 Loss : 8.867521365009814\n",
      "Iteration 25 Loss : 8.34345204094211\n",
      "Iteration 26 Loss : 7.850353118483743\n",
      "Iteration 27 Loss : 7.386397874602818\n",
      "Iteration 28 Loss : 6.94986173712617\n",
      "Iteration 29 Loss : 6.539124434950737\n",
      "Iteration 30 Loss : 6.1526621719118015\n",
      "Iteration 31 Loss : 5.789039869058961\n",
      "Iteration 32 Loss : 5.446907999417336\n",
      "Iteration 33 Loss : 5.124995576577539\n",
      "Iteration 34 Loss : 4.822108497170647\n",
      "Iteration 35 Loss : 4.537121521071987\n",
      "Iteration 36 Loss : 4.268978030723312\n",
      "Iteration 37 Loss : 4.01668121563854\n",
      "Iteration 38 Loss : 3.7792956126389763\n",
      "Iteration 39 Loss : 3.5559389510643094\n",
      "Iteration 40 Loss : 3.345782865003274\n",
      "Iteration 41 Loss : 3.1480471758404285\n",
      "Iteration 42 Loss : 2.961997679823884\n",
      "Iteration 43 Loss : 2.78694359065541\n",
      "Iteration 44 Loss : 2.622235303237792\n",
      "Iteration 45 Loss : 2.467261121418954\n",
      "Iteration 46 Loss : 2.321446092335641\n",
      "Iteration 47 Loss : 2.184248486806066\n",
      "Iteration 48 Loss : 2.0551593804914616\n",
      "Iteration 49 Loss : 1.9336995852420789\n",
      "Iteration 50 Loss : 1.8194178573235094\n",
      "Iteration 51 Loss : 1.7118903069357754\n",
      "Iteration 52 Loss : 1.6107175940030252\n",
      "Iteration 53 Loss : 1.5155241897377694\n",
      "Iteration 54 Loss : 1.4259567411109748\n",
      "Iteration 55 Loss : 1.3416826255281136\n",
      "Iteration 56 Loss : 1.262389208248047\n",
      "Iteration 57 Loss : 1.1877819791340551\n",
      "Iteration 58 Loss : 1.1175840765571434\n",
      "Iteration 59 Loss : 1.0515348500680068\n",
      "Iteration 60 Loss : 0.9893891461492582\n",
      "Iteration 61 Loss : 0.930916260625565\n",
      "Iteration 62 Loss : 0.875899078709395\n",
      "Iteration 63 Loss : 0.8241334819517507\n",
      "Iteration 64 Loss : 0.7754271861095672\n",
      "Iteration 65 Loss : 0.7295994320679934\n",
      "Iteration 66 Loss : 0.6864801042040583\n",
      "Iteration 67 Loss : 0.6459091389617334\n",
      "Iteration 68 Loss : 0.6077358933180028\n",
      "Iteration 69 Loss : 0.5718187120029812\n",
      "Iteration 70 Loss : 0.5380242202642829\n",
      "Iteration 71 Loss : 0.5062269967452033\n",
      "Iteration 72 Loss : 0.4763089781884024\n",
      "Iteration 73 Loss : 0.4481591180173807\n",
      "Iteration 74 Loss : 0.42167291418136477\n",
      "Iteration 75 Loss : 0.3967520449790852\n",
      "Iteration 76 Loss : 0.3733039992368791\n",
      "Iteration 77 Loss : 0.3512417316144445\n",
      "Iteration 78 Loss : 0.33048334753976116\n",
      "Iteration 79 Loss : 0.31095177724411444\n",
      "Iteration 80 Loss : 0.2925745286179104\n",
      "Iteration 81 Loss : 0.2752833763568879\n",
      "Iteration 82 Loss : 0.25901412505149535\n",
      "Iteration 83 Loss : 0.2437063914735247\n",
      "Iteration 84 Loss : 0.22930333977371198\n",
      "Iteration 85 Loss : 0.21575151284725816\n",
      "Iteration 86 Loss : 0.2030006012946216\n",
      "Iteration 87 Loss : 0.19100326852350488\n",
      "Iteration 88 Loss : 0.17971497196649536\n",
      "Iteration 89 Loss : 0.1690938194815031\n",
      "Iteration 90 Loss : 0.1591003719214838\n",
      "Iteration 91 Loss : 0.14969754273736763\n",
      "Iteration 92 Loss : 0.14085041966208015\n",
      "Iteration 93 Loss : 0.13252615564761738\n",
      "Iteration 94 Loss : 0.1246938532452423\n",
      "Iteration 95 Loss : 0.11732446503349986\n",
      "Iteration 96 Loss : 0.11039058885430607\n",
      "Iteration 97 Loss : 0.10386649785129919\n",
      "Iteration 98 Loss : 0.09772798570124883\n",
      "Iteration 99 Loss : 0.09195226348280558\n",
      "Iteration 100 Loss : 0.0865178816583512\n",
      "Iteration 101 Loss : 0.08140467291758889\n",
      "Iteration 102 Loss : 0.07659366262828358\n",
      "Iteration 103 Loss : 0.07206697005843195\n",
      "Iteration 104 Loss : 0.06780781192053903\n",
      "Iteration 105 Loss : 0.06380037696069592\n",
      "Iteration 106 Loss : 0.06002977345222309\n",
      "Iteration 107 Loss : 0.0564820075507719\n",
      "Iteration 108 Loss : 0.05314393144118542\n",
      "Iteration 109 Loss : 0.050003114234231524\n",
      "Iteration 110 Loss : 0.04704793686603195\n",
      "Iteration 111 Loss : 0.04426740148833972\n",
      "Iteration 112 Loss : 0.04165120020443161\n",
      "Iteration 113 Loss : 0.03918961375201954\n",
      "Iteration 114 Loss : 0.0368735034129829\n",
      "Iteration 115 Loss : 0.034694277992582755\n",
      "Iteration 116 Loss : 0.032643851730490094\n",
      "Iteration 117 Loss : 0.03071459534999028\n",
      "Iteration 118 Loss : 0.028899363239415818\n",
      "Iteration 119 Loss : 0.027191414181739672\n",
      "Iteration 120 Loss : 0.02558439994540113\n",
      "Iteration 121 Loss : 0.024072362337913877\n",
      "Iteration 122 Loss : 0.022649683089386127\n",
      "Iteration 123 Loss : 0.021311092099735786\n",
      "Iteration 124 Loss : 0.02005160424149179\n",
      "Iteration 125 Loss : 0.01886655505507656\n",
      "Iteration 126 Loss : 0.017751540667355833\n",
      "Iteration 127 Loss : 0.016702427744061103\n",
      "Iteration 128 Loss : 0.01571531497821091\n",
      "Iteration 129 Loss : 0.014786535770396103\n",
      "Iteration 130 Loss : 0.013912651762769943\n",
      "Iteration 131 Loss : 0.013090418519936803\n",
      "Iteration 132 Loss : 0.012316768931710837\n",
      "Iteration 133 Loss : 0.011588849600126475\n",
      "Iteration 134 Loss : 0.010903943586632107\n",
      "Iteration 135 Loss : 0.010259526183227799\n",
      "Iteration 136 Loss : 0.009653186757193668\n",
      "Iteration 137 Loss : 0.009082688171817357\n",
      "Iteration 138 Loss : 0.008545899068542421\n",
      "Iteration 139 Loss : 0.00804083320361364\n",
      "Iteration 140 Loss : 0.007565618804557518\n",
      "Iteration 141 Loss : 0.007118492429622391\n",
      "Iteration 142 Loss : 0.006697793120481266\n",
      "Iteration 143 Loss : 0.0063019473730584336\n",
      "Iteration 144 Loss : 0.005929501997799936\n",
      "Iteration 145 Loss : 0.005579070290327091\n",
      "Iteration 146 Loss : 0.005249347396309216\n",
      "Iteration 147 Loss : 0.004939114136252681\n",
      "Iteration 148 Loss : 0.004647215154254898\n",
      "Iteration 149 Loss : 0.00437256400626425\n",
      "Iteration 150 Loss : 0.004114139259196158\n",
      "Iteration 151 Loss : 0.0038709956233987848\n",
      "Iteration 152 Loss : 0.0036422222163822442\n",
      "Iteration 153 Loss : 0.0034269635873455254\n",
      "Iteration 154 Loss : 0.0032244300300798123\n",
      "Iteration 155 Loss : 0.003033866206344064\n",
      "Iteration 156 Loss : 0.0028545694817259646\n",
      "Iteration 157 Loss : 0.0026858615040063873\n",
      "Iteration 158 Loss : 0.002527124440860861\n",
      "Iteration 159 Loss : 0.002377772426750458\n",
      "Iteration 160 Loss : 0.0022372501846465924\n",
      "Iteration 161 Loss : 0.002105026221950533\n",
      "Iteration 162 Loss : 0.0019806188966821317\n",
      "Iteration 163 Loss : 0.001863566163059441\n",
      "Iteration 164 Loss : 0.0017534302886055876\n",
      "Iteration 165 Loss : 0.0016498016244949178\n",
      "Iteration 166 Loss : 0.0015522968336895225\n",
      "Iteration 167 Loss : 0.0014605572212372654\n",
      "Iteration 168 Loss : 0.0013742383231737623\n",
      "Iteration 169 Loss : 0.0012930183418168389\n",
      "Iteration 170 Loss : 0.0012166008279945002\n",
      "Iteration 171 Loss : 0.0011447005613673634\n",
      "Iteration 172 Loss : 0.0010770513341135804\n",
      "Iteration 173 Loss : 0.001013397095948145\n",
      "Iteration 174 Loss : 0.0009535029620325111\n",
      "Iteration 175 Loss : 0.0008971534673183893\n",
      "Iteration 176 Loss : 0.0008441301639000644\n",
      "Iteration 177 Loss : 0.0007942435095401501\n",
      "Iteration 178 Loss : 0.0007473036766382048\n",
      "Iteration 179 Loss : 0.0007031374518087182\n",
      "Iteration 180 Loss : 0.0006615806720993984\n",
      "Iteration 181 Loss : 0.0006224808039162045\n",
      "Iteration 182 Loss : 0.0005856932236775429\n",
      "Iteration 183 Loss : 0.0005510780772974099\n",
      "Iteration 184 Loss : 0.0005185112321657664\n",
      "Iteration 185 Loss : 0.00048786689510026934\n",
      "Iteration 186 Loss : 0.00045903387854597503\n",
      "Iteration 187 Loss : 0.00043190420223823955\n",
      "Iteration 188 Loss : 0.000406378034681195\n",
      "Iteration 189 Loss : 0.00038236074013664776\n",
      "Iteration 190 Loss : 0.0003597649139507893\n",
      "Iteration 191 Loss : 0.0003385032407062897\n",
      "Iteration 192 Loss : 0.00031849748027454767\n",
      "Iteration 193 Loss : 0.00029967346881992795\n",
      "Iteration 194 Loss : 0.0002819629431575354\n",
      "Iteration 195 Loss : 0.0002652991815966534\n",
      "Iteration 196 Loss : 0.00024961903501571355\n",
      "Iteration 197 Loss : 0.00023486641976601822\n",
      "Iteration 198 Loss : 0.00022098629075865584\n",
      "Iteration 199 Loss : 0.00020792651372860275\n",
      "Iteration 200 Loss : 0.00019563773612380077\n",
      "Final weights :  [-3.3990955  -0.20180899  0.80271349]\n",
      "Final bias  0.6009044964517248\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([-3.0,-1.0,2.0])\n",
    "bias = 1.0\n",
    "inputs = np.array([1.0,-2.0,3.0])\n",
    "target_output = 0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "for iteration in range(200):\n",
    "    linear_output = np.dot(weights,inputs) + bias\n",
    "    output = relu(linear_output)\n",
    "    loss = (output - target_output) ** 2\n",
    "\n",
    "    dloss_output = 2 * (output - target_output)\n",
    "    doutput_dlinear = relu_derivative(linear_output)\n",
    "    dlinear_weights = inputs\n",
    "    dlinear_bias = 1.0\n",
    "\n",
    "    dloss_dlinear = dloss_output * doutput_dlinear\n",
    "    dloss_dweights = dloss_dlinear * dlinear_weights\n",
    "    dloss_dbias = dloss_dlinear * dlinear_bias\n",
    "\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    bias -= learning_rate * dloss_dbias\n",
    "\n",
    "    print(f\"Iteration {iteration+1} Loss : {loss}\")\n",
    "\n",
    "print(\"Final weights : \", weights)\n",
    "print(\"Final bias \",bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation on an entire layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 , Loss : 466.56000000000006\n",
      "Iteration 21 , Loss : 5.329595763793193\n",
      "Iteration 41 , Loss : 0.41191524253483786\n",
      "Iteration 61 , Loss : 0.03183621475376345\n",
      "Iteration 81 , Loss : 0.002460565405431671\n",
      "Iteration 101 , Loss : 0.0001901729121621426\n",
      "Iteration 121 , Loss : 1.4698120139337557e-05\n",
      "Iteration 141 , Loss : 1.1359948840900371e-06\n",
      "Iteration 161 , Loss : 8.779778427447647e-08\n",
      "Iteration 181 , Loss : 6.785903626216421e-09\n",
      "Final weights : \n",
      " [[-0.00698895 -0.0139779  -0.02096685 -0.0279558 ]\n",
      " [ 0.25975286  0.11950571 -0.02074143 -0.16098857]\n",
      " [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\n",
      "Final bias  [-0.00698895 -0.04024714 -0.06451539]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = np.array([1.0,2.0,3.0,4.0])\n",
    "weights = np.array([[0.1,0.2,0.3,0.4],\n",
    "[0.5,0.6,0.7,0.8],\n",
    "[0.9,1.0,1.1,1.2]])\n",
    "\n",
    "biases = np.array([0.1,0.2,0.3])\n",
    "learning_rate = 0.001\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "for iteration in range(200):\n",
    "    z = np.dot(weights,inputs) + biases\n",
    "    a = relu(z)\n",
    "    y = np.sum(a)\n",
    "\n",
    "\n",
    "    loss = y**2\n",
    "    dL_dy = 2*y\n",
    "    dy_da = np.ones_like(a)\n",
    "    da_dz = relu_derivative(z)\n",
    "    dL_dz = dL_dy * dy_da * da_dz\n",
    "\n",
    "    dL_dW = np.outer(dL_dz,inputs)\n",
    "    dL_db = dL_dz\n",
    "\n",
    "    weights-= learning_rate * dL_dW\n",
    "    biases -= learning_rate * dL_db\n",
    "\n",
    "    if iteration % 20 ==0 :\n",
    "         print(f\"Iteration {iteration+1} , Loss : {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Final weights : \\n\", weights)\n",
    "print(\"Final bias \",biases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
